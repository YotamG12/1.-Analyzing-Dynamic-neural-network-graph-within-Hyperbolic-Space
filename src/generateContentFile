import pandas as pd
import ast
from sklearn.feature_extraction.text import CountVectorizer
import os
import sys
sys.stdout.reconfigure(encoding='utf-8')
# Load dataset
df = pd.read_csv("src/data/final_filtered_by_fos_and_reference.csv")

# Fill NaNs
df['abstract'] = df['abstract'].fillna("")
df['fos.name'] = df['fos.name'].fillna("Unknown")

# Build Bag-of-Words features from abstracts
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['abstract']).toarray()

# Normalize labels (e.g., "Computer science" -> "Computer_science")
labels = df['fos.name'].str.replace(" ", "_")

# Write dblpv13.content
content_path = "src/data/dblpv13.content"
with open(content_path, "w", encoding="utf-8") as f:
    for pid, row, label in zip(df['id'], X, labels):
        features_str = " ".join(map(str, row))
        f.write(f"{pid} {features_str} {label}\n")

print(f"✅ Created: {content_path}")

# Build set of valid paper IDs
paper_id_set = set(df['id'])

# Extract references
cites_path = "src/data/dblpv13.cites"
with open(cites_path, "w", encoding="utf-8") as f:
    for idx, row in df.iterrows():
        try:
            refs = ast.literal_eval(row['references'])
            for ref in refs:
                if ref in paper_id_set:
                    f.write(f"{row['id']} {ref}\n")
        except:
            continue

print(f"✅ Created: {cites_path}")
